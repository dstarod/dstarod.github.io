<!doctype html><html lang=ru-RU><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><title>Python NLTK + Stanford NLP | Круг интересов</title><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="Как известно, в Python стандартом работы с натуральным языком де-факто является NLTK. Несмотря на это, я довольно долго использовал Pattern от CLiPS из-за его простоты и скорости (многие отмечают тормознутость NLTK).
Но наступил момент, когда почти вся кодовая база была успешно портирована на Python 3.5, а разработчики Pattern так и не сделали версию с поддержкой третьей версии. И, судя по всему, не собираются.
Что-ж, будем использовать NLTK. От него мне нужны: токенизация, выделение POS (part-of-speech), получение N-grams и классификация твитов на группы с использованием Naive Bayes. Все это дело на Python 3.5."><meta name=generator content="Hugo 0.147.8"><meta name=robots content="index, follow"><link rel=stylesheet href=/ananke/css/main.min.8d048772ae72ab11245a0e296d1f2a36d3e3dd376c6c867394d6cc659c68fc37.css><link rel=canonical href=https://dstarod.github.io/python-nltk-stanford-nlp/><meta property="og:url" content="https://dstarod.github.io/python-nltk-stanford-nlp/"><meta property="og:site_name" content="Круг интересов"><meta property="og:title" content="Python NLTK + Stanford NLP"><meta property="og:description" content="Как известно, в Python стандартом работы с натуральным языком де-факто является NLTK. Несмотря на это, я довольно долго использовал Pattern от CLiPS из-за его простоты и скорости (многие отмечают тормознутость NLTK).
Но наступил момент, когда почти вся кодовая база была успешно портирована на Python 3.5, а разработчики Pattern так и не сделали версию с поддержкой третьей версии. И, судя по всему, не собираются.
Что-ж, будем использовать NLTK. От него мне нужны: токенизация, выделение POS (part-of-speech), получение N-grams и классификация твитов на группы с использованием Naive Bayes. Все это дело на Python 3.5."><meta property="og:locale" content="ru_RU"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2016-01-29T00:00:00+03:00"><meta property="article:modified_time" content="2016-01-29T00:00:00+03:00"><meta property="article:tag" content="Python"><meta property="article:tag" content="Nlp"><meta itemprop=name content="Python NLTK + Stanford NLP"><meta itemprop=description content="Как известно, в Python стандартом работы с натуральным языком де-факто является NLTK. Несмотря на это, я довольно долго использовал Pattern от CLiPS из-за его простоты и скорости (многие отмечают тормознутость NLTK).
Но наступил момент, когда почти вся кодовая база была успешно портирована на Python 3.5, а разработчики Pattern так и не сделали версию с поддержкой третьей версии. И, судя по всему, не собираются.
Что-ж, будем использовать NLTK. От него мне нужны: токенизация, выделение POS (part-of-speech), получение N-grams и классификация твитов на группы с использованием Naive Bayes. Все это дело на Python 3.5."><meta itemprop=datePublished content="2016-01-29T00:00:00+03:00"><meta itemprop=dateModified content="2016-01-29T00:00:00+03:00"><meta itemprop=wordCount content="482"><meta itemprop=keywords content="Python,Nlp"><meta name=twitter:card content="summary"><meta name=twitter:title content="Python NLTK + Stanford NLP"><meta name=twitter:description content="Как известно, в Python стандартом работы с натуральным языком де-факто является NLTK. Несмотря на это, я довольно долго использовал Pattern от CLiPS из-за его простоты и скорости (многие отмечают тормознутость NLTK).
Но наступил момент, когда почти вся кодовая база была успешно портирована на Python 3.5, а разработчики Pattern так и не сделали версию с поддержкой третьей версии. И, судя по всему, не собираются.
Что-ж, будем использовать NLTK. От него мне нужны: токенизация, выделение POS (part-of-speech), получение N-grams и классификация твитов на группы с использованием Naive Bayes. Все это дело на Python 3.5."></head><body class="ma0 avenir bg-near-white production"><header><div class=bg-black><nav class="pv3 ph3 ph4-ns" role=navigation><div class="flex-l center items-center justify-between"><a href class="f3 fw2 hover-white white-90 dib no-underline">Круг интересов</a><div class="flex-l items-center"><div class=ananke-socials></div></div></div></nav></div></header><main class=pb7 role=main><article class="flex-l mw8 center ph3 flex-wrap justify-between"><header class="mt4 w-100"><aside class="instapaper_ignoref b helvetica tracked ttu">Posts</aside><div id=sharing class="mt3 ananke-socials"></div><h1 class="f1 athelas mt3 mb1">Python NLTK + Stanford NLP</h1><time class="f6 mv4 dib tracked" datetime=2016-01-29T00:00:00+03:00>января 29, 2016</time></header><div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><p>Как известно, в Python стандартом работы с натуральным языком де-факто является NLTK. Несмотря на это, я довольно долго использовал Pattern от CLiPS из-за его простоты и скорости (многие отмечают тормознутость NLTK).</p><p>Но наступил момент, когда почти вся кодовая база была успешно портирована на Python 3.5, а разработчики Pattern так и не сделали версию с поддержкой третьей версии. И, судя по всему, не собираются.</p><p>Что-ж, будем использовать NLTK. От него мне нужны: токенизация, выделение POS (part-of-speech), получение N-grams и классификация твитов на группы с использованием Naive Bayes. Все это дело на Python 3.5.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pip3 install nltk
</span></span></code></pre></div><p>Модули для NLTK устанавливаются так:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> nltk
</span></span><span style=display:flex><span>nltk<span style=color:#f92672>.</span>download()
</span></span></code></pre></div><p>&mldr; или так:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>python3 -m nltk.downloader punkt averaged_perceptron_tagger
</span></span><span style=display:flex><span><span style=color:#75715e># python3 -m nltk.downloader all</span>
</span></span></code></pre></div><p>Теперь немного примеров.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>tweet <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;&#39;&#39;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Kinto by Mozilla - An open source Parse alternative &gt;&gt;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    https://github.com/Kinto/kinto/ #python #parse
</span></span></span><span style=display:flex><span><span style=color:#e6db74>&#39;&#39;&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Получаем токены. Стандартный универсальный метод:</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> nltk <span style=color:#f92672>import</span> word_tokenize
</span></span><span style=display:flex><span>print(word_tokenize(tweet))
</span></span><span style=display:flex><span><span style=color:#e6db74>&#39;&#39;&#39;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>[
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#39;Kinto&#39;, &#39;by&#39;, &#39;Mozilla&#39;, &#39;-&#39;, &#39;An&#39;, &#39;open&#39;, &#39;source&#39;,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#39;Parse&#39;, &#39;alternative&#39;,&#39;&gt;&#39;, &#39;&gt;&#39;, &#39;https&#39;, &#39;:&#39;,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#39;//github.com/Kinto/kinto/&#39;, &#39;#&#39;, &#39;python&#39;, &#39;#&#39;, &#39;parse&#39;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>]
</span></span></span><span style=display:flex><span><span style=color:#e6db74>&#39;&#39;&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># С использованием специализированного класса</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> nltk.tokenize <span style=color:#f92672>import</span> TweetTokenizer
</span></span><span style=display:flex><span><span style=color:#75715e># Убрать имена пользователей и многократно повторяемые символы</span>
</span></span><span style=display:flex><span>tt <span style=color:#f92672>=</span> TweetTokenizer(preserve_case<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, reduce_len<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, strip_handles<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>print(tt<span style=color:#f92672>.</span>tokenize(tweet))
</span></span><span style=display:flex><span><span style=color:#e6db74>&#39;&#39;&#39;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>[
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#39;Kinto&#39;, &#39;by&#39;, &#39;Mozilla&#39;, &#39;-&#39;, &#39;An&#39;, &#39;open&#39;,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#39;source&#39;, &#39;Parse&#39;, &#39;alternative&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#39;https://github.com/Kinto/kinto/&#39;, &#39;#python&#39;, &#39;#parse&#39;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>]
</span></span></span><span style=display:flex><span><span style=color:#e6db74>&#39;&#39;&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Возьмем немного почищеные токены. Воспользуемся обычным токенизатором</span>
</span></span><span style=display:flex><span>tokens <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>    t <span style=color:#66d9ef>for</span> t <span style=color:#f92672>in</span> word_tokenize(tweet)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> len(t) <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>and</span> <span style=color:#f92672>not</span> t<span style=color:#f92672>.</span>startswith(<span style=color:#e6db74>&#39;http&#39;</span>)
</span></span><span style=display:flex><span>        <span style=color:#f92672>and</span> <span style=color:#f92672>not</span> t<span style=color:#f92672>.</span>startswith(<span style=color:#e6db74>&#39;/&#39;</span>)
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span><span style=color:#e6db74>&#39;&#39;&#39;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>[
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#39;Kinto&#39;, &#39;by&#39;, &#39;Mozilla&#39;, &#39;An&#39;, &#39;open&#39;, &#39;source&#39;,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#39;Parse&#39;, &#39;alternative&#39;, &#39;python&#39;, &#39;parse&#39;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>]
</span></span></span><span style=display:flex><span><span style=color:#e6db74>&#39;&#39;&#39;</span>
</span></span></code></pre></div><p>Токены получили, теперь будем узнавать части речи. Можно использовать встроенный метод, а можно пойти интересным путем, и привлечь Stanford NLP библиотеки. Для этого скачаем и распакуем каталог с ними в нашу рабочую директорию, где пишем код. В Python укажем место расположения JAR и модулей через переменные окружения.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> nltk.tag <span style=color:#f92672>import</span> StanfordPOSTagger
</span></span><span style=display:flex><span>os<span style=color:#f92672>.</span>environ[<span style=color:#e6db74>&#39;CLASSPATH&#39;</span>] <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(
</span></span><span style=display:flex><span>    os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>curdir, <span style=color:#e6db74>&#39;stanford-postagger-2015-04-20&#39;</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>os<span style=color:#f92672>.</span>environ[<span style=color:#e6db74>&#39;STANFORD_MODELS&#39;</span>] <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(
</span></span><span style=display:flex><span>    os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>curdir, <span style=color:#e6db74>&#39;stanford-postagger-2015-04-20&#39;</span>, <span style=color:#e6db74>&#39;models&#39;</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>stanford_tagger <span style=color:#f92672>=</span> StanfordPOSTagger(<span style=color:#e6db74>&#39;english-bidirectional-distsim.tagger&#39;</span>)
</span></span></code></pre></div><p>Теперь можно сравнить результаты методов:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> nltk <span style=color:#f92672>import</span> pos_tag
</span></span><span style=display:flex><span>print(pos_tag(tokens))
</span></span><span style=display:flex><span>print(stanford_tagger<span style=color:#f92672>.</span>tag(tokens))
</span></span></code></pre></div><p>Результаты почти идентичны, за исключением &ldquo;An&rdquo;. StanfordPOSTagger считает его существительным. Допустим, мы выберем последний вариант. Найдем все имена существительные (NN, NNS, NNP, NNP-PERS, NNP-ORG):</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>tagger <span style=color:#f92672>=</span> stanford_tagger<span style=color:#f92672>.</span>tag
</span></span><span style=display:flex><span>nouns <span style=color:#f92672>=</span> [word<span style=color:#f92672>.</span>lower() <span style=color:#66d9ef>for</span> word, pos <span style=color:#f92672>in</span> tagger(tokens) <span style=color:#66d9ef>if</span> pos<span style=color:#f92672>.</span>startswith(<span style=color:#e6db74>&#39;NN&#39;</span>)]
</span></span><span style=display:flex><span>print(nouns)  <span style=color:#75715e># [&#39;kinto&#39;, &#39;mozilla&#39;, &#39;an&#39;, &#39;source&#39;, &#39;parse&#39;, &#39;python&#39;, &#39;parse&#39;]</span>
</span></span></code></pre></div><p>Теперь N-grams. Не проблема сделать самостоятельно, но в NLTK уже есть пара готовых функций.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> nltk.util <span style=color:#f92672>import</span> everygrams, ngrams
</span></span><span style=display:flex><span>print(list(ngrams(nouns, <span style=color:#ae81ff>2</span>)))
</span></span><span style=display:flex><span><span style=color:#75715e># [(&#39;kinto&#39;, &#39;mozilla&#39;), (&#39;mozilla&#39;, &#39;an&#39;), ...  (&#39;python&#39;, &#39;parse&#39;)]</span>
</span></span><span style=display:flex><span>print(list(everygrams(nouns, min_len<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, max_len<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>)))
</span></span><span style=display:flex><span><span style=color:#e6db74>&#39;&#39;&#39;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>[
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    (&#39;kinto&#39;, &#39;mozilla&#39;), (&#39;mozilla&#39;, &#39;an&#39;), ... (&#39;python&#39;, &#39;parse&#39;),
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    (&#39;kinto&#39;, &#39;mozilla&#39;, &#39;an&#39;), (&#39;mozilla&#39;, &#39;an&#39;, &#39;source&#39;),
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    ... (&#39;parse&#39;, &#39;python&#39;, &#39;parse&#39;)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>]
</span></span></span><span style=display:flex><span><span style=color:#e6db74>&#39;&#39;&#39;</span>
</span></span></code></pre></div><p>Как я уже говорил, NLTK порой очень медленный. С использованием Stanford библиотек он медленнее в разы. Существенно улучшает ситуацию обработка больших объемов твитов разом, а не один за другим.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> nltk <span style=color:#f92672>import</span> pos_tag, pos_tag_sents
</span></span><span style=display:flex><span>tokens_group <span style=color:#f92672>=</span> [tokens <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>100</span>)]
</span></span><span style=display:flex><span>print(pos_tag_sents(tokens_group))  <span style=color:#75715e># 0.3 sec</span>
</span></span><span style=display:flex><span>print(stanford_tagger<span style=color:#f92672>.</span>tag_sents(tokens_group))  <span style=color:#75715e># 6.4 sec !!</span>
</span></span></code></pre></div><p>На сём откланиваюсь.</p><ul class=pa0><li class="list di"><a href=/tags/python/ class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">Python</a></li><li class="list di"><a href=/tags/nlp/ class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">Nlp</a></li></ul><div class="mt6 instapaper_ignoref"></div></div><aside class="w-30-l mt6-l"></aside></article></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white white-70 dn dib-ns pv2 ph3 no-underline" href>&copy; Круг интересов 2025</a><div><div class=ananke-socials></div></div></div></footer></body></html>